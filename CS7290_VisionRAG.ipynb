{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeenBean024/VisionRAG/blob/main/CS7290_VisionRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTu_iCUNyFuT",
        "outputId": "0c6562e8-38ab-42f2-d22c-3680d45a631f"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install poppler-utils\n",
        "!pip install colpali-engine\n",
        "!pip install pdf2image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "X7Tg7_T8xmlQ",
        "outputId": "17e9b383-a393-45bd-ce5e-6e6aea0939a5"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from typing import Any, List, cast\n",
        "import h5py\n",
        "from colpali_engine.utils.torch_utils import get_torch_device\n",
        "from peft import LoraConfig\n",
        "from PIL import Image\n",
        "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
        "from transformers.models.qwen2_vl import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "import torch\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "import traceback\n",
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyY2ZId-yRul"
      },
      "outputs": [],
      "source": [
        "class ColQwen2ForRAG(ColQwen2):\n",
        "    \"\"\"\n",
        "    ColQwen2 model implementation that can be used both for retrieval and generation.\n",
        "    Allows switching between retrieval and generation modes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._is_retrieval_enabled = True\n",
        "\n",
        "    def forward(self, *args, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Forward pass that calls either Qwen2VLForConditionalGeneration.forward for generation\n",
        "        or ColQwen2.forward for retrieval based on the current mode.\n",
        "        \"\"\"\n",
        "        if self.is_retrieval_enabled:\n",
        "            return ColQwen2.forward(self, *args, **kwargs)\n",
        "        else:\n",
        "            return Qwen2VLForConditionalGeneration.forward(self, *args, **kwargs)\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Generate text using Qwen2VLForConditionalGeneration.generate.\n",
        "        \"\"\"\n",
        "        if not self.is_generation_enabled:\n",
        "            raise ValueError(\n",
        "                \"Set the model to generation mode by calling `enable_generation()` before calling `generate()`.\"\n",
        "            )\n",
        "        return super().generate(*args, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def is_retrieval_enabled(self) -> bool:\n",
        "        return self._is_retrieval_enabled\n",
        "\n",
        "    @property\n",
        "    def is_generation_enabled(self) -> bool:\n",
        "        return not self.is_retrieval_enabled\n",
        "\n",
        "    def enable_retrieval(self) -> None:\n",
        "        \"\"\"\n",
        "        Switch to retrieval mode.\n",
        "        \"\"\"\n",
        "        self.enable_adapters()\n",
        "        self._is_retrieval_enabled = True\n",
        "\n",
        "    def enable_generation(self) -> None:\n",
        "        \"\"\"\n",
        "        Switch to generation mode.\n",
        "        \"\"\"\n",
        "        self.disable_adapters()\n",
        "        self._is_retrieval_enabled = False\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "uHy6Z-IlxxOb",
        "outputId": "cd377ea4-1186-4e75-8a55-fb5dcf7fabc3"
      },
      "outputs": [],
      "source": [
        "################### Setup Model and Processor ############################\n",
        "model_name = \"vidore/colqwen2-v1.0\"\n",
        "device = get_torch_device(\"auto\")\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "omWHVDS7yu6W",
        "outputId": "f870aa54-c9b9-472d-ccb8-9161ab770ba4"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig.from_pretrained(model_name)\n",
        "\n",
        "# Load the processors\n",
        "processor_retrieval = cast(ColQwen2Processor, ColQwen2Processor.from_pretrained(model_name))\n",
        "processor_generation = cast(Qwen2VLProcessor, Qwen2VLProcessor.from_pretrained(lora_config.base_model_name_or_path))\n",
        "\n",
        "model = cast(\n",
        "    ColQwen2ForRAG,\n",
        "    ColQwen2ForRAG.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=device,\n",
        "    ),\n",
        ")\n",
        "model = model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1V9z0wMcyyi3",
        "outputId": "fdb6034d-b8a7-4223-f89e-dd8c72a08ca5"
      },
      "outputs": [],
      "source": [
        "IMAGE_HEIGHT = 1024\n",
        "RETRIEVE_K = 2\n",
        "BATCH_SIZE = 1\n",
        "SCALE_IMAGE = True\n",
        "KNOWLEDGE_BASE = \"knowledge_base.h5\"\n",
        "ROW_SIZE = 128  # The last dimension is fixed\n",
        "\n",
        "# Define a fixed-size array dtype for a row of embedding (128 float32s)\n",
        "row_dtype = np.dtype((np.float32, (ROW_SIZE,)))\n",
        "# Create a variable-length dtype based on that fixed-size type:\n",
        "vlen_float_dtype = h5py.special_dtype(vlen=np.dtype('float32'))\n",
        "\n",
        "print(f\"Using image scaling: {SCALE_IMAGE}\")\n",
        "print(f\"Using batch size: {BATCH_SIZE}\")\n",
        "print(f\"Using retrieve k: {RETRIEVE_K}\")\n",
        "print(f\"Using image height: {IMAGE_HEIGHT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxFzrDWUzc_r"
      },
      "outputs": [],
      "source": [
        "# def process_pdf(file_path):\n",
        "#     \"\"\"\n",
        "#     Process a PDF file by extracting pages and running them through ColQwen2 for retrieval.\n",
        "#     Uses a generator to reduce peak RAM usage.\n",
        "#     \"\"\"\n",
        "#     print(\"Starting PDF to image conversion\")\n",
        "\n",
        "#     # Convert PDF pages to images one at a time to save memory\n",
        "#     images_gen = convert_from_path(file_path)\n",
        "\n",
        "#     page_embeddings = []\n",
        "\n",
        "#     for i, image in tqdm(enumerate(images_gen)):\n",
        "#         # Scale each image immediately to reduce memory footprint\n",
        "#         if SCALE_IMAGE:\n",
        "#             image = scale_image(image, new_height=IMAGE_HEIGHT)\n",
        "\n",
        "#         model.enable_retrieval()\n",
        "#         data = processor_retrieval.process_images([image])\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             batch_doc = {k: v.to(model.device) for k, v in data.items()}\n",
        "#             embedding = model(**batch_doc).cpu().float().numpy()\n",
        "#         print(f\"Embedding shape - {embedding.shape}\")\n",
        "#         page_embeddings.append(embedding)\n",
        "\n",
        "#         # Cleanup memory\n",
        "#         del data, batch_doc, embedding, image\n",
        "#         torch.cuda.empty_cache()\n",
        "#         gc.collect()\n",
        "\n",
        "#     # Concatenate embeddings from all pages\n",
        "#     return images_gen, np.concatenate(page_embeddings, axis=0)\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Process a PDF file page by page, extract embeddings, and store embeddings and images in HDF5.\n",
        "    Each page's embedding is expected to have shape (1, N, 128); after dropping the batch dimension,\n",
        "    we get (N, 128). We then flatten it to a 1D array (of length N*128) and store the number N in a separate dataset.\n",
        "    \"\"\"\n",
        "    print(\"Starting PDF processing...\")\n",
        "    images_gen = convert_from_path(file_path)\n",
        "\n",
        "    with h5py.File(KNOWLEDGE_BASE, 'a') as f:\n",
        "        filename = os.path.basename(file_path)\n",
        "        if filename in f:\n",
        "            del f[filename]\n",
        "        group = f.create_group(filename)\n",
        "\n",
        "        # Create extendable dataset for flattened embeddings.\n",
        "        vector_dataset = group.create_dataset(\n",
        "            \"vectors\",\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=vlen_float_dtype,\n",
        "            compression=\"gzip\"\n",
        "        )\n",
        "        # Dataset to store the number of rows (N) for each page's embedding.\n",
        "        shape_dataset = group.create_dataset(\n",
        "            \"vector_shapes\",\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=np.int32,\n",
        "            compression=\"gzip\"\n",
        "        )\n",
        "        # Create extendable dataset for images as variable-length arrays of bytes.\n",
        "        image_dtype = h5py.special_dtype(vlen=np.dtype('uint8'))\n",
        "        image_dataset = group.create_dataset(\n",
        "            \"images\",\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=image_dtype\n",
        "        )\n",
        "        model.enable_retrieval()\n",
        "\n",
        "        for i, image in tqdm(enumerate(images_gen), desc=\"Processing pages\"):\n",
        "            if SCALE_IMAGE:\n",
        "                image = scale_image(image, new_height=IMAGE_HEIGHT)\n",
        "\n",
        "            data = processor_retrieval.process_images([image])\n",
        "            with torch.no_grad():\n",
        "                batch_doc = {k: v.to(model.device) for k, v in data.items()}\n",
        "                embedding = model(**batch_doc).cpu().float().numpy()\n",
        "            # Drop the redundant batch dimension: expected shape becomes (N, 128)\n",
        "            embedding = embedding[0]\n",
        "            print(f\"Embedding shape - {embedding.shape}\")  # e.g. (779, 128)\n",
        "\n",
        "            # Flatten the embedding: (N, 128) -> (N*128,)\n",
        "            flat_embedding = embedding.flatten()\n",
        "\n",
        "            # Convert image to compressed JPEG bytes\n",
        "            img_buffer = BytesIO()\n",
        "            image.save(img_buffer, format=\"JPEG\", quality=75)\n",
        "            img_bytes = np.frombuffer(img_buffer.getvalue(), dtype=np.uint8)\n",
        "\n",
        "            # Extend datasets dynamically\n",
        "            vector_dataset.resize((i + 1,))\n",
        "            shape_dataset.resize((i + 1,))\n",
        "            image_dataset.resize((i + 1,))\n",
        "\n",
        "            vector_dataset[i] = flat_embedding.tolist()\n",
        "            shape_dataset[i] = embedding.shape[0]  # number of rows N\n",
        "            image_dataset[i] = img_bytes\n",
        "\n",
        "            # Cleanup\n",
        "            del data, batch_doc, embedding, flat_embedding, image, img_buffer, img_bytes\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    print(f\"PDF processing complete. Data stored in group '{filename}'\")\n",
        "\n",
        "def process_query(query):\n",
        "    \"\"\"\n",
        "    Process a query by running it through ColQwen2 for retrieval.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The embedding of the query as a tensor with shape (1, embedding_dim).\n",
        "    \"\"\"\n",
        "    data = processor_retrieval.process_queries([query])\n",
        "    model.enable_retrieval()\n",
        "    with torch.no_grad():\n",
        "        batch_doc = {k: v.to(model.device) for k, v in data.items()}\n",
        "        embeddings_doc = model(**batch_doc)\n",
        "    return embeddings_doc.cpu().float().numpy()\n",
        "\n",
        "def get_embeddings_by_filename(filename):\n",
        "    with h5py.File(KNOWLEDGE_BASE, 'r') as f:\n",
        "        if filename not in f:\n",
        "            raise ValueError(f\"Filename {filename} not found in knowledge base.\")\n",
        "        group = f[filename]\n",
        "        vectors = group['vectors']\n",
        "        shapes = group['vector_shapes']\n",
        "        embeddings = []\n",
        "        # Reconstruct each page's embedding from the flattened data.\n",
        "        for i in range(vectors.shape[0]):\n",
        "            flat_embedding = np.array(vectors[i], dtype=np.float32)\n",
        "            # Retrieve the number of rows (N) stored for this page.\n",
        "            N = int(shapes[i])\n",
        "            # Reshape flat_embedding (of length N*128) to (N, 128)\n",
        "            embedding = flat_embedding.reshape(N, ROW_SIZE)\n",
        "            embeddings.append(embedding)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "def get_image_by_filename(filename, page_index):\n",
        "    with h5py.File(KNOWLEDGE_BASE, 'r') as f:\n",
        "        if filename not in f:\n",
        "            raise ValueError(f\"Filename {filename} not found in knowledge base.\")\n",
        "        # Retrieve image bytes for the specified page.\n",
        "        img_bytes = bytes(f[filename]['images'][page_index])\n",
        "        return Image.open(BytesIO(img_bytes))\n",
        "\n",
        "\n",
        "# def get_data_by_filename(filename):\n",
        "#     \"\"\"\n",
        "#     Retrieve vector and image data from the knowledge base HDF5 file for a given filename.\n",
        "\n",
        "#     Args:\n",
        "#         filename (str): The name of the file to look up in the HDF5 knowledge base.\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: A tuple containing the vector data and image data if the filename is found,\n",
        "#                or None if the filename is not present in the knowledge base.\n",
        "#     \"\"\"\n",
        "\n",
        "#     with h5py.File(os.path.join('data','knowledge_base.h5'), 'r') as f:\n",
        "#         if filename in f:\n",
        "#             return f[filename]['vector'][()], f[filename]['image'][()]\n",
        "#         else:\n",
        "#             raise ValueError(f\"Filename {filename} not found in knowledge base.\")\n",
        "\n",
        "def scale_image(image: Image.Image, new_height: int = 1024) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Scale an image to a new height while maintaining the aspect ratio.\n",
        "    \"\"\"\n",
        "    width, height = image.size\n",
        "    aspect_ratio = width / height\n",
        "    new_width = int(new_height * aspect_ratio)\n",
        "\n",
        "    return image.resize((new_width, new_height))\n",
        "\n",
        "# def store_vector(filename, vector_data, image_data):\n",
        "#     \"\"\"\n",
        "#     Stores the vector data for a given filename in an HDF5 file called 'vectors.h5'.\n",
        "\n",
        "#     Args:\n",
        "#         filename (str): The filename of the PDF file.\n",
        "#         vector_data (list): The vector data to store.\n",
        "\n",
        "#     Returns:\n",
        "#         None\n",
        "#     \"\"\"\n",
        "#     os.makedirs('data', exist_ok=True)\n",
        "#     with h5py.File(os.path.join('data','knowledge_base.h5'), 'a') as f:\n",
        "#         # Create a group for each filename if it doesn't exist\n",
        "#         if filename not in f:\n",
        "#             group = f.create_group(filename)\n",
        "#         else:\n",
        "#             group = f[filename]\n",
        "\n",
        "#         # Store the vector data\n",
        "#         if 'vector' in group:\n",
        "#             del group['vector']  # Delete existing dataset if it exists\n",
        "\n",
        "#         # Store the vector data\n",
        "#         if 'image' in group:\n",
        "#             del group['image']  # Delete existing dataset if it exists\n",
        "#         group.create_dataset('vector', data=vector_data)\n",
        "#         group.create_dataset('image', data=image_data)\n",
        "#     print(f\"Stored vector data for {filename} in knowledge base.\")\n",
        "\n",
        "def generate_answer(query, images):\n",
        "    \"\"\"\n",
        "    Generates an answer to a given query using input images and a Retrieval-Augmented Generation (RAG) model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The question or query that needs to be answered.\n",
        "        images (torch.Tensor): The tensor containing image data to be used for answering the query.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of generated text responses based on the input query and images.\n",
        "    \"\"\"\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                *[\n",
        "                    {\"type\": \"image\"}\n",
        "                    for _ in images\n",
        "                ],\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": f\"Answer the following question using the input images: {query}\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    text_prompt = processor_generation.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "    inputs_generation = processor_generation(\n",
        "        text=[text_prompt],\n",
        "        images=images,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate the RAG response\n",
        "    model.enable_generation()\n",
        "    output_ids = model.generate(**inputs_generation, max_new_tokens=100)\n",
        "\n",
        "    # Ensure that only the newly generated token IDs are retained from output_ids\n",
        "    generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs_generation.input_ids, output_ids)]\n",
        "\n",
        "    # Decode the RAG response\n",
        "    output_text = processor_generation.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "\n",
        "    del inputs_generation, output_ids, generated_ids\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return output_text\n",
        "\n",
        "def ndarray_to_base64(ndarr: np.ndarray, image_format: str = 'png') -> str:\n",
        "    \"\"\"\n",
        "    Convert a NumPy ndarray (image) to a base64 encoded string.\n",
        "\n",
        "    Args:\n",
        "        ndarr (np.ndarray): The input image as a NumPy array.\n",
        "        image_format (str): Format to encode the image (default is 'png').\n",
        "\n",
        "    Returns:\n",
        "        str: The base64 encoded string of the image.\n",
        "    \"\"\"\n",
        "    # Encode the numpy array into the specified image format\n",
        "    success, buffer = cv2.imencode(f'.{image_format}', ndarr)\n",
        "    if not success:\n",
        "        raise ValueError(\"Failed to encode image\")\n",
        "\n",
        "    # Convert buffer to bytes and then encode to base64 string\n",
        "    img_bytes = buffer.tobytes()\n",
        "    base64_str = base64.b64encode(img_bytes).decode('utf-8')\n",
        "    return base64_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnAaHacjz-Cg"
      },
      "source": [
        "## Upload PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-oVFYhXvz9kG",
        "outputId": "31dc3970-3202-4c2b-8b5d-48168164d4be"
      },
      "outputs": [],
      "source": [
        "# @title Upload PDF to knowledge base {\"vertical-output\":true}\n",
        "file_path = \"/content/03_chubb_limited_annual_report_2023.pdf\" # @param {\"type\":\"string\"}\n",
        "os.makedirs('uploads', exist_ok=True)\n",
        "\n",
        "# Process with ColQwen2 retrieval model\n",
        "process_pdf(file_path)\n",
        "\n",
        "# Store in HDF5\n",
        "# store_vector(os.path.basename(file_path), page_embeddings, images)\n",
        "\n",
        "# del images, page_embeddings\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUY18MSEmpRH"
      },
      "outputs": [],
      "source": [
        "# query = \"What is the name of the company?\"\n",
        "# filename = \"Chubb-Limited-Investor-Presentation-December-2023-short.pdf\"\n",
        "# query_embedding = process_query(query)\n",
        "# image_embedding = get_embeddings_by_filename(filename)\n",
        "# image_embedding = get_embeddings_by_filename(filename)\n",
        "# # image_embedding, images = get_data_by_filename(filename)\n",
        "# scores = processor_retrieval.score_multi_vector(\n",
        "#       torch.from_numpy(query_embedding),\n",
        "#       torch.from_numpy(image_embedding)\n",
        "#   )\n",
        "# top_pages = scores.numpy()[0].argsort()[-RETRIEVE_K:][::-1]\n",
        "# print(f'Top pages: {top_pages}')\n",
        "# # images = [get_image_by_filename(filename, page) for page in top_pages]\n",
        "# # print(f'Images: {type(images)}')\n",
        "# # answer = generate_answer(query, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "de65855fc1a54060b84112f3b5cc2427",
            "6cd2bfef0aa84147a3343fc7868f8e0b",
            "3a0ea0b219144ad4873f2d99fef90dde",
            "48906f905c1f4993a63629a1b416bb7e",
            "1ddfc46b1d5b405db93954963d0e190c",
            "b4914869c7594741ab7a715067fa2a07",
            "07a0453c1d1843b8a6e1415880bf7e32",
            "1ef3032bb9394188ad9abc363960950e",
            "76fde9d9163c492f9de63eb63a4560dd"
          ]
        },
        "id": "mHR4NLxV1uE4",
        "outputId": "1fea05ce-b488-43d0-d8b7-36fe3be40b3a"
      },
      "outputs": [],
      "source": [
        "filenames = ['a', 'b', 'c']\n",
        "try:\n",
        "    with h5py.File(KNOWLEDGE_BASE, 'r') as f:\n",
        "        filenames = list(f.keys())\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create dropdown for file selection\n",
        "file_picker = widgets.Dropdown(options=filenames, description='Select File:')\n",
        "\n",
        "# Create text input box\n",
        "text_input = widgets.Text(description='Input Query:')\n",
        "\n",
        "# Create submit button\n",
        "submit_button = widgets.Button(description='Submit')\n",
        "\n",
        "def on_submit(b):\n",
        "  print(f'Selected file: {file_picker.value}')\n",
        "  print(f'User input: {text_input.value}')\n",
        "  query = text_input.value\n",
        "  filename = file_picker.value\n",
        "  try:\n",
        "    query_embedding = process_query(query)\n",
        "    image_embedding = get_embeddings_by_filename(filename)\n",
        "    # image_embedding, images = get_data_by_filename(filename)\n",
        "    scores = processor_retrieval.score_multi_vector(\n",
        "          torch.from_numpy(query_embedding),\n",
        "          torch.from_numpy(image_embedding)\n",
        "      )\n",
        "    top_pages = scores.numpy()[0].argsort()[-RETRIEVE_K:][::-1]\n",
        "    print(f'Top pages: {top_pages}')\n",
        "    images = [get_image_by_filename(filename, page) for page in top_pages]\n",
        "    print(f'Images: {type(images)}')\n",
        "    answer = generate_answer(query, images)\n",
        "    print(f'Answer: {answer}')\n",
        "  except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "    traceback.print_exc()\n",
        "\n",
        "submit_button.on_click(on_submit)\n",
        "\n",
        "# Display widgets\n",
        "display(file_picker, text_input, submit_button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD8-TCZ-VBok"
      },
      "source": [
        "## Image retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDLtAUCIanm9",
        "outputId": "6eb69b33-bc19-4279-8c29-229d73cc7684"
      },
      "outputs": [],
      "source": [
        "dir(processor_retrieval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwjQf9xhwqM1"
      },
      "outputs": [],
      "source": [
        "def process_image(image):\n",
        "    \"\"\"\n",
        "    Process a image by running it through ColQwen2 for retrieval.\n",
        "\n",
        "    Args:\n",
        "        image : The image.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The embedding of the image as a tensor with shape (1, embedding_dim).\n",
        "    \"\"\"\n",
        "    data = processor_retrieval.process_images(image)\n",
        "    model.enable_retrieval()\n",
        "    with torch.no_grad():\n",
        "        batch_doc = {k: v.to(model.device) for k, v in data.items()}\n",
        "        embeddings_doc = model(**batch_doc)\n",
        "    return embeddings_doc.cpu().float().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "tyOBCK5dVvwx",
        "outputId": "97910fd3-6f7f-4c75-bcee-8ded973e8046"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(\"/content/judy gonsalez.jpeg\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnaqqeNOVzul",
        "outputId": "643be852-4d27-4a9b-e5e1-b446fbabc260"
      },
      "outputs": [],
      "source": [
        "image_embedding = process_image([image])\n",
        "print(image_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsrXH2bsV8Nf",
        "outputId": "88be029c-0e73-417a-ea1b-8dab7d403545"
      },
      "outputs": [],
      "source": [
        "filename = \"03_chubb_limited_annual_report_2023.pdf\"\n",
        "query_embedding = image_embedding\n",
        "image_embedding = get_embeddings_by_filename(filename)\n",
        "# image_embedding, images = get_data_by_filename(filename)\n",
        "scores = processor_retrieval.score_multi_vector(\n",
        "      torch.from_numpy(query_embedding),\n",
        "      torch.from_numpy(image_embedding)\n",
        "  )\n",
        "top_pages = scores.numpy()[0].argsort()[-RETRIEVE_K:][::-1]\n",
        "print(f'Top pages: {top_pages}')\n",
        "# images = [get_image_by_filename(filename, page) for page in top_pages]\n",
        "# print(f'Images: {type(images)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqj_gr9rbFCh",
        "outputId": "749852c1-0458-4086-a093-b2f4ad71b932"
      },
      "outputs": [],
      "source": [
        "scores.numpy()[0][30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa7xzZvSYbvF"
      },
      "outputs": [],
      "source": [
        "images = [get_image_by_filename(filename, page) for page in top_pages]\n",
        "for image in images:\n",
        "  print(image)\n",
        "print(f'Images: {type(images)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4yPZu8YUyO"
      },
      "outputs": [],
      "source": [
        "answer = generate_answer(query, images)\n",
        "print(f'Answer: {answer}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPBS/DqgKHUeOIV4O0oRWUF",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07a0453c1d1843b8a6e1415880bf7e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1ef3032bb9394188ad9abc363960950e",
            "style": "IPY_MODEL_76fde9d9163c492f9de63eb63a4560dd",
            "tooltip": ""
          }
        },
        "1ddfc46b1d5b405db93954963d0e190c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef3032bb9394188ad9abc363960950e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0ea0b219144ad4873f2d99fef90dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48906f905c1f4993a63629a1b416bb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Input Query:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1ddfc46b1d5b405db93954963d0e190c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b4914869c7594741ab7a715067fa2a07",
            "value": ""
          }
        },
        "6cd2bfef0aa84147a3343fc7868f8e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76fde9d9163c492f9de63eb63a4560dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b4914869c7594741ab7a715067fa2a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de65855fc1a54060b84112f3b5cc2427": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Chubb-Limited-Investor-Presentation-December-2023-short.pdf",
              "Chubb-Limited-Investor-Presentation-December-2023.pdf"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select File:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_6cd2bfef0aa84147a3343fc7868f8e0b",
            "style": "IPY_MODEL_3a0ea0b219144ad4873f2d99fef90dde"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
